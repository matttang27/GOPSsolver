from __future__ import annotations

import argparse
import json
import math
import sys
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path
from typing import Any, Mapping

import numpy as np

ROOT = Path(__file__).resolve().parents[1]
AI_DIR = str(ROOT / "ai")
REPORTS_DIR = str(ROOT / "reports")
for path in (AI_DIR, REPORTS_DIR):
    if path not in sys.path:
        sys.path.insert(0, path)

from common import (
    build_matrix,
    cache_objective,
    cmp,
    compress_cards,
    list_cards,
    load_evc,
    load_meta,
    remove_card,
)
from linprog import findBestStrategy


PolicyName = str
ObjectiveName = str
StateKey = tuple[int, int, int, int, int]


@dataclass
class EvalStats:
    br_states: int = 0
    br_cache_hits: int = 0
    policy_states: int = 0
    policy_cache_hits: int = 0
    lp_solves: int = 0


def highest_card(mask: int) -> int:
    if mask == 0:
        return 0
    return int(mask.bit_length())


def count_above(mask: int, threshold: int) -> int:
    if threshold <= 0:
        return int(mask.bit_count())
    if threshold >= 16:
        return 0
    higher = mask & ~((1 << threshold) - 1)
    return int(higher.bit_count())


@lru_cache(maxsize=1 << 16)
def guarantee_values(prize_mask: int) -> tuple[int, ...]:
    if prize_mask == 0:
        return (0,)
    sorted_prizes: list[int] = []
    total = 0
    for card in range(16, 0, -1):
        if prize_mask & (1 << (card - 1)):
            sorted_prizes.append(card)
            total += card
    out = [-total]
    sum_top = 0
    for card in sorted_prizes:
        sum_top += card
        out.append((2 * sum_top) - total)
    return tuple(out)


def infer_n_from_meta(meta: Any) -> int | None:
    if not isinstance(meta, dict):
        return None
    run = meta.get("run")
    if not isinstance(run, dict):
        return None
    value = run.get("N")
    if isinstance(value, int) and value > 0:
        return value
    return None


def normalize_distribution(
    actions: list[int],
    weights: np.ndarray,
    eps: float,
) -> tuple[list[int], np.ndarray]:
    probs = np.asarray(weights, dtype=np.float64)
    if probs.ndim != 1 or probs.size != len(actions):
        raise ValueError("Probability vector shape mismatch.")
    probs = np.maximum(probs, 0.0)
    mask = probs > eps
    if not bool(np.any(mask)):
        idx = int(np.argmax(probs)) if probs.size > 0 else 0
        mask = np.zeros_like(probs, dtype=bool)
        if probs.size > 0:
            mask[idx] = True
    out_actions = [card for card, keep in zip(actions, mask) if keep]
    out_probs = probs[mask]
    total = float(out_probs.sum())
    if total <= 0.0:
        out_probs = np.full(out_probs.shape, 1.0 / max(out_probs.size, 1), dtype=np.float64)
    else:
        out_probs /= total
    return out_actions, out_probs


def policy_distribution_builtin(
    policy: PolicyName,
    cards_a: list[int],
    cur_p: int,
) -> tuple[list[int], np.ndarray]:
    n = len(cards_a)
    if n == 0:
        raise ValueError("Empty hand.")
    if policy == "random":
        return cards_a, np.full(n, 1.0 / n, dtype=np.float64)
    if policy == "highest":
        best = max(cards_a)
        return [best], np.array([1.0], dtype=np.float64)
    if policy == "lowest":
        best = min(cards_a)
        return [best], np.array([1.0], dtype=np.float64)
    if policy == "current":
        if cur_p in cards_a:
            return [cur_p], np.array([1.0], dtype=np.float64)
        return cards_a, np.full(n, 1.0 / n, dtype=np.float64)
    if policy == "exploit-current":
        target = cur_p + 1
        if target in cards_a:
            return [target], np.array([1.0], dtype=np.float64)
        return cards_a, np.full(n, 1.0 / n, dtype=np.float64)
    raise ValueError(f"Unsupported built-in policy: {policy}")


class ExploitabilityEvaluator:
    def __init__(
        self,
        *,
        policy: PolicyName,
        eval_objective: ObjectiveName,
        policy_eps: float,
        strategy_cache: Mapping[int, float] | None,
        n: int,
        enable_compression: bool = True,
        use_exploit_guarantee: bool = True,
    ) -> None:
        self.policy = policy
        self.eval_objective = eval_objective
        self.policy_eps = max(float(policy_eps), 0.0)
        self.n = int(n)
        self.max_total_diff = (self.n * (self.n + 1)) // 2
        self.diff_axis = np.arange(-self.max_total_diff, self.max_total_diff + 1, dtype=np.int32)
        self.strategy_cache = strategy_cache
        self.strategy_objective = cache_objective(strategy_cache) if strategy_cache is not None else "win"
        self.policy_depends_on_diff = (self.policy == "evc-ne" and self.strategy_objective == "win")
        # "current" and "exploit-current" depend on absolute card labels, so do not compress there.
        if policy in {"current", "exploit-current"}:
            self.enable_compression = False
        else:
            self.enable_compression = bool(enable_compression)
        self.use_exploit_guarantee = bool(use_exploit_guarantee)
        self.br_cache: dict[StateKey, float] = {}
        self.br_vec_cache: dict[tuple[int, int, int, int], np.ndarray] = {}
        self.policy_cache: dict[StateKey, tuple[list[int], np.ndarray]] = {}
        self.stats = EvalStats()

    def _normalize_state(self, a: int, b: int) -> tuple[int, int]:
        if not self.enable_compression:
            return a, b
        return compress_cards(a, b)

    def _policy_key(self, a: int, b: int, p: int, diff: int, cur_p: int) -> StateKey:
        if self.policy == "evc-ne" and self.strategy_objective == "points":
            diff = 0
        return (a, b, p, diff, cur_p)

    def _br_key(self, a: int, b: int, p: int, diff: int, cur_p: int) -> StateKey:
        if self.eval_objective == "points":
            diff = 0
        return (a, b, p, diff, cur_p)

    def _exploit_side_guaranteed_win(self, a: int, b: int, p: int, diff: int, cur_p: int) -> bool:
        if self.eval_objective != "win" or not self.use_exploit_guarantee:
            return False
        prize_mask = p
        if cur_p > 0:
            prize_mask |= (1 << (cur_p - 1))
        values = guarantee_values(prize_mask)
        prize_count = len(values) - 1
        if prize_count <= 0:
            return False
        max_a = highest_card(a)
        guarantee_b = count_above(b, max_a)
        if guarantee_b > prize_count:
            guarantee_b = prize_count
        return (diff - values[guarantee_b]) < 0

    def _shift_with_win_saturation(self, values: np.ndarray, delta: int) -> np.ndarray:
        if delta == 0:
            return values
        out = np.empty_like(values)
        if delta > 0:
            out[:-delta] = values[delta:]
            out[-delta:] = 1.0
        else:
            k = -delta
            out[k:] = values[:-k]
            out[:k] = -1.0
        return out

    def _br_value_vec(self, a: int, b: int, p: int, cur_p: int) -> np.ndarray:
        a, b = self._normalize_state(a, b)
        key = (a, b, p, cur_p)
        cached = self.br_vec_cache.get(key)
        if cached is not None:
            self.stats.br_cache_hits += 1
            return cached

        cards_b = list_cards(b)
        prizes = list_cards(p)
        actions_a, probs_a = self.policy_distribution(a, b, p, 0, cur_p)
        action_probs = [(int(card_a), float(prob_a)) for card_a, prob_a in zip(actions_a, probs_a) if float(prob_a) > 0.0]

        prize_count = len(prizes)
        b_vectors: list[np.ndarray] = []
        for card_b in cards_b:
            new_b = remove_card(b, card_b)
            ev_vec = np.zeros_like(self.diff_axis, dtype=np.float64)
            for card_a, prob_a in action_probs:
                new_a = remove_card(a, card_a)
                round_delta = cmp(card_a, card_b) * cur_p
                if prize_count == 0:
                    outcome = np.sign(self.diff_axis + round_delta).astype(np.float64)
                else:
                    cont = np.zeros_like(self.diff_axis, dtype=np.float64)
                    for next_prize in prizes:
                        child_p = remove_card(p, next_prize)
                        child_vec = self._br_value_vec(new_a, new_b, child_p, next_prize)
                        cont += self._shift_with_win_saturation(child_vec, round_delta)
                    cont /= prize_count
                    outcome = cont
                ev_vec += prob_a * outcome
            b_vectors.append(ev_vec)

        value = np.minimum.reduce(b_vectors)
        self.br_vec_cache[key] = value
        self.stats.br_states += 1
        return value

    def policy_distribution(self, a: int, b: int, p: int, diff: int, cur_p: int) -> tuple[list[int], np.ndarray]:
        a, b = self._normalize_state(a, b)
        key = self._policy_key(a, b, p, diff, cur_p)
        cached = self.policy_cache.get(key)
        if cached is not None:
            self.stats.policy_cache_hits += 1
            return cached

        cards_a = list_cards(a)
        if self.policy == "evc-ne":
            if self.strategy_cache is None:
                raise ValueError("evc-ne policy requires a cache.")
            if len(cards_a) == 1:
                result = ([cards_a[0]], np.array([1.0], dtype=np.float64))
            else:
                mat = build_matrix(self.strategy_cache, a, b, p, diff, cur_p)
                if not mat:
                    raise RuntimeError(
                        "Failed to build matrix for policy state: "
                        f"A={cards_a} B={list_cards(b)} P={list_cards(p)} diff={diff} curP={cur_p}"
                    )
                p_a, _v = findBestStrategy(np.array(mat, dtype=np.float64))
                if p_a is None:
                    raise RuntimeError(
                        "LP failed while computing policy at state: "
                        f"A={cards_a} B={list_cards(b)} P={list_cards(p)} diff={diff} curP={cur_p}"
                    )
                self.stats.lp_solves += 1
                result = normalize_distribution(cards_a, p_a, self.policy_eps)
        else:
            result = policy_distribution_builtin(self.policy, cards_a, cur_p)

        self.policy_cache[key] = result
        self.stats.policy_states += 1
        return result

    def br_value(self, a: int, b: int, p: int, diff: int, cur_p: int) -> float:
        if self.eval_objective == "win" and not self.policy_depends_on_diff:
            vec = self._br_value_vec(a, b, p, cur_p)
            idx = int(diff + self.max_total_diff)
            if idx < 0:
                return -1.0
            if idx >= vec.size:
                return 1.0
            return float(vec[idx])

        a, b = self._normalize_state(a, b)
        key = self._br_key(a, b, p, diff, cur_p)
        cached = self.br_cache.get(key)
        if cached is not None:
            self.stats.br_cache_hits += 1
            return cached

        if self._exploit_side_guaranteed_win(a, b, p, diff, cur_p):
            self.br_cache[key] = -1.0
            self.stats.br_states += 1
            return -1.0

        cards_b = list_cards(b)
        prizes = list_cards(p)
        actions_a, probs_a = self.policy_distribution(a, b, p, diff, cur_p)

        best_for_b = math.inf
        prize_count = len(prizes)
        for card_b in cards_b:
            new_b = remove_card(b, card_b)
            ev_a_given_b = 0.0

            for card_a, prob_a in zip(actions_a, probs_a):
                if prob_a <= 0.0:
                    continue
                new_a = remove_card(a, card_a)
                round_delta = cmp(card_a, card_b) * cur_p

                if prize_count == 0:
                    if self.eval_objective == "win":
                        ev_a = float(cmp(diff + round_delta, 0))
                    else:
                        ev_a = float(round_delta)
                else:
                    child_diff = (diff + round_delta) if self.eval_objective == "win" else 0
                    cont = 0.0
                    for next_prize in prizes:
                        child_p = remove_card(p, next_prize)
                        cont += self.br_value(new_a, new_b, child_p, child_diff, next_prize)
                    cont /= prize_count
                    if self.eval_objective == "points":
                        ev_a = float(round_delta) + cont
                    else:
                        ev_a = cont

                ev_a_given_b += float(prob_a) * ev_a

            if ev_a_given_b < best_for_b:
                best_for_b = ev_a_given_b

        self.br_cache[key] = best_for_b
        self.stats.br_states += 1
        return best_for_b


def evaluate_average_exploitability(
    *,
    n: int,
    evaluator: ExploitabilityEvaluator,
) -> dict[str, Any]:
    if n <= 0 or n > 16:
        raise ValueError("n must be between 1 and 16.")
    full = (1 << n) - 1
    per_curp: dict[int, float] = {}
    total = 0.0
    for cur_p in range(1, n + 1):
        remaining = remove_card(full, cur_p)
        diff0 = 0
        value_a = evaluator.br_value(full, full, remaining, diff0, cur_p)
        per_curp[cur_p] = value_a
        total += value_a
    avg_value_a = total / n
    exploitability = -avg_value_a
    return {
        "n": n,
        "avg_value_a_under_best_response": avg_value_a,
        "avg_max_exploitability": exploitability,
        "per_initial_prize_value_a": per_curp,
    }


def main() -> None:
    parser = argparse.ArgumentParser(
        description=(
            "Compute average maximum exploitability for a fixed GOPS policy using "
            "exact recursive best response with memoization."
        )
    )
    parser.add_argument(
        "--policy",
        choices=["evc-ne", "random", "highest", "lowest", "current", "exploit-current"],
        default="evc-ne",
        help="Policy being exploited.",
    )
    parser.add_argument(
        "--cache",
        help="Path to .evc cache (required when --policy evc-ne).",
    )
    parser.add_argument(
        "--eval-objective",
        choices=["auto", "win", "points"],
        default="auto",
        help="Objective used when evaluating best response payoff.",
    )
    parser.add_argument("--n", type=int, help="Number of cards (1..16).")
    parser.add_argument(
        "--policy-eps",
        type=float,
        default=1e-12,
        help="Trim policy support below this probability and renormalize.",
    )
    parser.add_argument(
        "--no-compression",
        action="store_true",
        help="Disable A/B card compression in exploitability recursion.",
    )
    parser.add_argument(
        "--no-exploit-guarantee",
        action="store_true",
        help="Disable exploitative-side guaranteed-win shortcut (win objective only).",
    )
    parser.add_argument("--json-out", help="Optional output path for JSON report.")
    args = parser.parse_args()

    strategy_cache: Mapping[int, float] | None = None
    strategy_obj = "win"
    meta = None
    if args.policy == "evc-ne":
        if not args.cache:
            raise SystemExit("--cache is required for --policy evc-ne")
        strategy_cache = load_evc(args.cache)
        strategy_obj = cache_objective(strategy_cache)
        meta = load_meta(args.cache)

    n = args.n
    if n is None:
        n = infer_n_from_meta(meta)
    if n is None:
        raise SystemExit("Could not infer n. Please pass --n explicitly.")

    eval_objective = args.eval_objective
    if eval_objective == "auto":
        eval_objective = strategy_obj if args.policy == "evc-ne" else "win"

    enable_compression = not bool(args.no_compression)
    use_exploit_guarantee = not bool(args.no_exploit_guarantee)

    evaluator = ExploitabilityEvaluator(
        policy=args.policy,
        eval_objective=eval_objective,
        policy_eps=args.policy_eps,
        strategy_cache=strategy_cache,
        n=n,
        enable_compression=enable_compression,
        use_exploit_guarantee=use_exploit_guarantee,
    )
    result = evaluate_average_exploitability(n=n, evaluator=evaluator)

    out = {
        "policy": args.policy,
        "strategy_objective": strategy_obj if args.policy == "evc-ne" else None,
        "eval_objective": eval_objective,
        "policy_eps": float(max(args.policy_eps, 0.0)),
        "compression": evaluator.enable_compression,
        "exploit_guarantee": use_exploit_guarantee,
        "vectorized_win_diff_solver": (eval_objective == "win" and not evaluator.policy_depends_on_diff),
        **result,
        "stats": {
            "br_states": evaluator.stats.br_states,
            "br_cache_hits": evaluator.stats.br_cache_hits,
            "policy_states": evaluator.stats.policy_states,
            "policy_cache_hits": evaluator.stats.policy_cache_hits,
            "policy_lp_solves": evaluator.stats.lp_solves,
        },
    }

    print(f"policy: {out['policy']}")
    print(f"strategy objective: {out['strategy_objective']}")
    print(f"evaluation objective: {out['eval_objective']}")
    print(f"compression: {out['compression']}")
    print(f"exploit-side guarantee: {out['exploit_guarantee']}")
    print(f"vectorized win solver: {out['vectorized_win_diff_solver']}")
    print(f"n: {out['n']}")
    print(f"avg value for exploited player (A): {out['avg_value_a_under_best_response']:+.6f}")
    print(f"avg maximum exploitability (-A value): {out['avg_max_exploitability']:+.6f}")
    print("per initial prize value (A):")
    for cur_p, value in sorted(out["per_initial_prize_value_a"].items()):
        print(f"  curP={cur_p}: {value:+.6f}")
    print("stats:")
    print(f"  br states solved: {out['stats']['br_states']}")
    print(f"  br cache hits: {out['stats']['br_cache_hits']}")
    print(f"  policy states solved: {out['stats']['policy_states']}")
    print(f"  policy cache hits: {out['stats']['policy_cache_hits']}")
    print(f"  policy LP solves: {out['stats']['policy_lp_solves']}")

    if args.json_out:
        with open(args.json_out, "w", encoding="utf-8") as f:
            json.dump(out, f, indent=2)
        print(f"saved: {args.json_out}")


if __name__ == "__main__":
    main()
